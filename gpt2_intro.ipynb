{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "gpt2_intro.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GSmUd25aOYIW",
        "colab_type": "text"
      },
      "source": [
        "# This is the Jupyter Notebook ran in Google Colab during the GPT-2 Workshop at the Data Science Minneapolis Event on January 25th, 2020."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QCQEoZn2M3Sg",
        "colab_type": "text"
      },
      "source": [
        "We need to pip install the gpt-2-simple Python library. This is a wrapper around the source code that OpenAI published via GitHub."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kESDAdOhM5eu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install -q gpt-2-simple"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q1D5Zof1NE5S",
        "colab_type": "text"
      },
      "source": [
        "Now that we've pip installed the library, we can import it into the Python Notebook."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UFU96HNPJ1Ws",
        "colab_type": "code",
        "outputId": "dcf3d7f2-de05-4561-b9fe-123a616e4ed8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 199
        }
      },
      "source": [
        "import gpt_2_simple as gpt2"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:\n",
            "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "  * https://github.com/tensorflow/io (for I/O related ops)\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZjdtedptNKmi",
        "colab_type": "text"
      },
      "source": [
        "Here we can decide which model size we want to download and use, 1558M is the largest model, 774M is the second largest. The larger the model, the longer it takes to run the inference and download/load. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KEN1bm2iJ2yg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model_size = \"1558M\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ACiAraJaNYUl",
        "colab_type": "text"
      },
      "source": [
        "We need to download the model to the file storage system."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0qGAY_ivKs9h",
        "colab_type": "code",
        "outputId": "cc001781-79c2-43b6-e958-cee76f40ac90",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "gpt2.download_gpt2(model_name=model_size)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fetching checkpoint: 1.05Mit [00:00, 533Mit/s]                                                      \n",
            "Fetching encoder.json: 1.05Mit [00:00, 130Mit/s]                                                    \n",
            "Fetching hparams.json: 1.05Mit [00:00, 768Mit/s]                                                    \n",
            "Fetching model.ckpt.data-00000-of-00001: 6.23Git [02:05, 49.8Mit/s]                                 \n",
            "Fetching model.ckpt.index: 1.05Mit [00:00, 399Mit/s]                                                \n",
            "Fetching model.ckpt.meta: 2.10Mit [00:00, 194Mit/s]                                                 \n",
            "Fetching vocab.bpe: 1.05Mit [00:00, 241Mit/s]                                                       \n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7qi1ZD_wNc90",
        "colab_type": "text"
      },
      "source": [
        "Now we need to start the Tensorflow session to allow us to run the Tensorflow model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LLvkjmezJ4Pf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sess = gpt2.start_tf_sess()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EDFLe1pNNhvY",
        "colab_type": "text"
      },
      "source": [
        "With the model downloaded and stored in the local file storage, we can load the model into memory to use."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QrsWP0luJ5uh",
        "colab_type": "code",
        "outputId": "f02fccb6-da70-4a3b-cb9d-8a49ba381bdf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "gpt2.load_gpt2(sess, model_name = model_size)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading pretrained model models/1558M/model.ckpt\n",
            "INFO:tensorflow:Restoring parameters from models/1558M/model.ckpt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cv7qZK-nNpFT",
        "colab_type": "text"
      },
      "source": [
        "Here we can send the text we want the model to continue off of. This will be sent into the prefix parameter of the `generate()` function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ONpw-Tl9J8bR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "text = \"Data Science Minneapolis is ecstatic from the generous contributions from Google Colab's department\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v5nftYPnN297",
        "colab_type": "text"
      },
      "source": [
        "`generate()` is the function we use to generate the text. We need to pass the Tensorflow session, model we selected, and the text we want to continue off of to the `prefix` parameter."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aV4XS8SwKAc6",
        "colab_type": "code",
        "outputId": "4c469cd3-91e8-4b79-a4bb-dcf07b111048",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 207
        }
      },
      "source": [
        "gpt2.generate(\n",
        "    sess,\n",
        "    model_name = model_size,\n",
        "    prefix = text,\n",
        "    length = 100,\n",
        "    temperature = 0.7,\n",
        "    top_p = 0.9,\n",
        "    nsamples = 1,\n",
        "    batch_size = 1\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/gpt_2_simple/src/sample.py:32: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "Data Science Minneapolis is ecstatic from the generous contributions from Google Colab's department of Data Science and Analytics.\n",
            "\n",
            "This is just the latest in a series of initiatives for the city, including the Google Fiber, the first city in the United States to have gigabit internet speeds.\n",
            "\n",
            "The Google Fiber project has resulted in a number of innovative uses for the fiber, including a data center in the heart of downtown Minneapolis, which is also home to the University of Minnesota.\n",
            "\n",
            "The latest announcement is a new partnership with the Minneapolis Institute of Arts and the University of Minnesota\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}